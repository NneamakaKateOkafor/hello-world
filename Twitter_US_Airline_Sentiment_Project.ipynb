{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter US Airline Sentiment Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOrEtP9KQeddvf4H33X7VEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NneamakaKateOkafor/hello-world/blob/master/Twitter_US_Airline_Sentiment_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bISyGCvRd-F1"
      },
      "source": [
        "Twitter US Airline Sentiment Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGl-mcSgeMo7"
      },
      "source": [
        "**Data Description:**\r\n",
        "\r\n",
        "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from\r\n",
        "February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed\r\n",
        "by categorizing negative reasons (such as \"late flight\" or \"rude service\").\r\n",
        "bold text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gguyGG6ehbd"
      },
      "source": [
        "**1. Import the libraries, load dataset, print shape of data, data description. (5 Marks)**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdF2_8VOejVt"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ80GBoakV1a"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHXM5n4rivP8"
      },
      "source": [
        "import nltk\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import re, string, unicodedata\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('wordnet')\r\n",
        "import contractions\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\r\n",
        "from nltk.stem import WordNetLemmatizer, LancasterStemmer\r\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\r\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,plot_confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VwEsicMete8"
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8nCjyWwgwVV"
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XZPSIJQg1JU"
      },
      "source": [
        "# Define the pathway to the folder where the csv file is stored\r\n",
        "project_path = '/content/drive/My Drive/Colab Notebooks/'\r\n",
        "\r\n",
        "dataset_file = project_path + 'Tweets.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJUj-w8mlYbB"
      },
      "source": [
        "# Read the csv file\r\n",
        "mydata = pd.read_csv(dataset_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI2hQtSymqlK"
      },
      "source": [
        "# Basic Information of the dataset including data types\r\n",
        "mydata.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcZSQzL2mr2R"
      },
      "source": [
        "# Check only the data types not the whole information\r\n",
        "mydata.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pbVLCSImsiD"
      },
      "source": [
        "# Analysis of the body of distributions / head\r\n",
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-s9LhBmg0aD"
      },
      "source": [
        "# Display the shape of the data\r\n",
        "mydata.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpcwzu9Kg0B3"
      },
      "source": [
        "# Describe the attributes of this dataset (name, range of values observed, mean and median, standard deviation and quartiles)\r\n",
        "mydata.describe().transpose()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLogVKLwgzOL"
      },
      "source": [
        "# Number of unique values in each column\r\n",
        "mydata.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZrgRFxamX7p"
      },
      "source": [
        "# Check for duplicate rows\r\n",
        "mydata.duplicated().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEUCKr2L-RDj"
      },
      "source": [
        "# To remove duplicates, we can use the following code, but i will not remove them since all sentiments in this project are required.\r\n",
        "# mydata = mydata.drop_duplicates()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oPDdH3AmW8K"
      },
      "source": [
        "# Skewness of the dataset\r\n",
        "mydata.skew()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x06EE8QZmVjY"
      },
      "source": [
        "# Null values\r\n",
        "mydata.isnull().sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyhinN_7ez5P"
      },
      "source": [
        "**2. Understand of data-columns: (5 Marks)**\r\n",
        "\r\n",
        "a. Drop all other columns except “text” and “airline_sentiment”\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnVbcHXCe1sF"
      },
      "source": [
        "# Create a copy of mydata\r\n",
        "mydata2 = mydata.copy()\r\n",
        "mydata2.drop(['tweet_id','airline_sentiment_confidence','negativereason','negativereason_confidence',\r\n",
        "              'airline','airline_sentiment_gold','name',\r\n",
        "              'negativereason_gold','retweet_count','tweet_coord','tweet_created',\r\n",
        "              'tweet_location','user_timezone'], axis=1, inplace=True)\r\n",
        "\r\n",
        "mydata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn46MSk4qMy5"
      },
      "source": [
        "b. Check the shape of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3fN_5Kpl8X"
      },
      "source": [
        "# Display the shape of the data after dropping the columns\r\n",
        "mydata2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ntWfZR_mlM"
      },
      "source": [
        "**Note: The columns reduced from 15 to 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiknP2RxqYDP"
      },
      "source": [
        "\r\n",
        "c. Print first 5 rows of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOnWAT8ie9h0"
      },
      "source": [
        "# Display full dataframe information (Non-truncated Text column)\r\n",
        "pd.set_option('display.max_colwidth', None)\r\n",
        "mydata2.head"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYQWS6nLfB6d"
      },
      "source": [
        "**3. Text pre-processing: Data preparation. (20 Marks)**\r\n",
        "\r\n",
        "a. Html tag removal.\r\n",
        "\r\n",
        "b. Tokenization.\r\n",
        "\r\n",
        "c. Remove the numbers.\r\n",
        "\r\n",
        "d. Removal of Special Characters and Punctuations.\r\n",
        "\r\n",
        "e. Conversion to lowercase.\r\n",
        "\r\n",
        "f. Lemmatize or stemming.\r\n",
        "\r\n",
        "g. Join the words in the list to convert back to text string in the dataframe. (So that each row\r\n",
        "contains the data in text format.)\r\n",
        "\r\n",
        "h. Print first 5 rows of data after pre-processing.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3BPW3nVfOLk"
      },
      "source": [
        "# HTML tag removal\r\n",
        "def remove_html_tags(text):\r\n",
        "  soup = BeautifulSoup(text, 'html.parser')\r\n",
        "  return soup.get_text()\r\n",
        "\r\n",
        "mydata2['text'] = mydata2['text'].apply(lambda x: remove_html_tags(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJKvcN_Ts8vg"
      },
      "source": [
        "mydata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H01rcBVdrsGf"
      },
      "source": [
        "# Remove the numbers\r\n",
        "def remove_numbers(text):\r\n",
        "  text = re.sub(r'\\d+', '', str(text))\r\n",
        "  return text\r\n",
        "\r\n",
        "\r\n",
        "mydata2['text'] = mydata2['text'].apply(lambda x: remove_numbers(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otgeL5Ij7Mrr"
      },
      "source": [
        "mydata2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMYSe5Pfrrpg"
      },
      "source": [
        "# Removal of Special Characters and Punctuations\r\n",
        "def remove_punts_characters(text):\r\n",
        "  new_text = []\r\n",
        "  clean_txt = re.sub(r'[#,@,&,?,!]', '', text)\r\n",
        "  clean_txt = contractions.fix(clean_txt)\r\n",
        "  #new_txt = re.sub(r'[^\\w\\s]', ',', clean_txt)\r\n",
        "  #if new_txt != '':\r\n",
        "    #new_text.append(new_txt)\r\n",
        "  \r\n",
        "  return clean_txt\r\n",
        "\r\n",
        "mydata2['text'] = mydata2['text'].apply(lambda x: remove_punts_characters(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6BKSOjX7Qq7"
      },
      "source": [
        "mydata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fG4RXcRWMib_"
      },
      "source": [
        "# Tokenization\r\n",
        "mydata2['text'] = mydata2.apply(lambda x: nltk.word_tokenize(x['text']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUSHodPFNCme"
      },
      "source": [
        "mydata2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX4Vn4jQMj1-"
      },
      "source": [
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXOSWfyUMkSV"
      },
      "source": [
        "stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqnHsk35rrPQ"
      },
      "source": [
        "# Conversion to lowercase\r\n",
        "def to_lowercase(text):\r\n",
        "  new_text = []\r\n",
        "  for txt in text:\r\n",
        "    new_txt = txt.lower()\r\n",
        "    new_text.append(new_txt)\r\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHxUAVqC7YHD"
      },
      "source": [
        "# Remove non-ASCII\r\n",
        "def remove_non_ascii(text):\r\n",
        "  \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\r\n",
        "  new_text = []\r\n",
        "  for txt in text:\r\n",
        "    new_txt = unicodedata.normalize('NFKD',txt).encode('ascii', 'ignore').decode('utf-8','ignore')\r\n",
        "    new_text.append(new_txt)\r\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm-b1x2e7bD8"
      },
      "source": [
        "# Remove Punctuation\r\n",
        "def remove_punctuation(text):\r\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\r\n",
        "    new_text = []\r\n",
        "    for txt in text:\r\n",
        "        new_txt = re.sub(r'[^\\w\\s]', '', text)\r\n",
        "        if new_txt != '':\r\n",
        "            new_text.append(new_txt)\r\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBbvIP_u7daD"
      },
      "source": [
        "# Remove Stopwords\r\n",
        "def remove_stopwords(text):\r\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\r\n",
        "    new_text = []\r\n",
        "    for txt in text:\r\n",
        "        if txt not in stopwords:\r\n",
        "          new_text.append(txt)\r\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHztk5Gwrqy4"
      },
      "source": [
        "# Lemmatize or stemming\r\n",
        "def lemmatize_list(text):\r\n",
        "   lemmatizer = WordNetLemmatizer()\r\n",
        "   new_text = []\r\n",
        "   for txt in text:\r\n",
        "     new_text.append(lemmatizer.lemmatize(txt))\r\n",
        "   return new_text\r\n",
        "\r\n",
        "def stemming_list(text):\r\n",
        "   new_text = []\r\n",
        "   for txt in text:\r\n",
        "     new_text.append(LancasterStemmer.stem(txt))\r\n",
        "\r\n",
        "   return new_text \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg9jwmr9rqVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "28bd51a3-aa6e-4c72-ad84-99930e615292"
      },
      "source": [
        "# Join the words in the list to convert back to text string in the dataframe\r\n",
        "def normalize(text):\r\n",
        "  text = remove_non_ascii(text)\r\n",
        "  text = to_lowercase(text)\r\n",
        " # text = remove_punctuation(text)\r\n",
        "  text = remove_stopwords(text)\r\n",
        "  text = lemmatize_list(text)\r\n",
        "  return '.'.join(text)\r\n",
        "\r\n",
        "mydata2['text'] = mydata2.apply(lambda x: normalize(x['text']), axis=1)\r\n",
        "mydata2"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>virginamerica.dhepburn.said..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>virginamerica.plus.added.commercial.experience.....tacky..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>virginamerica.today.....must.mean.need.take.another.trip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>virginamerica.really.aggressive.blast.obnoxious.``.entertainment.''.guest.'.face.little.recourse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>virginamerica.really.big.bad.thing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>positive</td>\n",
              "      <td>americanair.thank.got.different.flight.chicago..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14636</th>\n",
              "      <td>negative</td>\n",
              "      <td>americanair.leaving.minute.late.flight...warning.communication.minute.late.flight...called.shitty.customer.svc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14637</th>\n",
              "      <td>neutral</td>\n",
              "      <td>americanair.please.bring.american.airline.blackberry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14638</th>\n",
              "      <td>negative</td>\n",
              "      <td>americanair.money.change.flight.answer.phone.suggestion.make.commitment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14639</th>\n",
              "      <td>neutral</td>\n",
              "      <td>americanair.ppl.need.know.many.seat.next.flight...plz.put.u.standby.people.next.flight</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14640 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      airline_sentiment                                                                                                            text\n",
              "0               neutral                                                                                   virginamerica.dhepburn.said..\n",
              "1              positive                                                      virginamerica.plus.added.commercial.experience.....tacky..\n",
              "2               neutral                                                        virginamerica.today.....must.mean.need.take.another.trip\n",
              "3              negative                virginamerica.really.aggressive.blast.obnoxious.``.entertainment.''.guest.'.face.little.recourse\n",
              "4              negative                                                                              virginamerica.really.big.bad.thing\n",
              "...                 ...                                                                                                             ...\n",
              "14635          positive                                                                americanair.thank.got.different.flight.chicago..\n",
              "14636          negative  americanair.leaving.minute.late.flight...warning.communication.minute.late.flight...called.shitty.customer.svc\n",
              "14637           neutral                                                            americanair.please.bring.american.airline.blackberry\n",
              "14638          negative                                         americanair.money.change.flight.answer.phone.suggestion.make.commitment\n",
              "14639           neutral                          americanair.ppl.need.know.many.seat.next.flight...plz.put.u.standby.people.next.flight\n",
              "\n",
              "[14640 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtuWhGowrp0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "efd5ebc5-8d18-4dc1-f7bb-bf6f39de9e11"
      },
      "source": [
        "# Print first 5 rows of data after pre-processing\r\n",
        "mydata2.head()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>virginamerica.dhepburn.said..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>virginamerica.plus.added.commercial.experience.....tacky..</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>virginamerica.today.....must.mean.need.take.another.trip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>virginamerica.really.aggressive.blast.obnoxious.``.entertainment.''.guest.'.face.little.recourse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>virginamerica.really.big.bad.thing</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment                                                                                              text\n",
              "0           neutral                                                                     virginamerica.dhepburn.said..\n",
              "1          positive                                        virginamerica.plus.added.commercial.experience.....tacky..\n",
              "2           neutral                                          virginamerica.today.....must.mean.need.take.another.trip\n",
              "3          negative  virginamerica.really.aggressive.blast.obnoxious.``.entertainment.''.guest.'.face.little.recourse\n",
              "4          negative                                                                virginamerica.really.big.bad.thing"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe75JGiBfQBY"
      },
      "source": [
        "**4. Vectorization: (10 Marks)**\r\n",
        "\r\n",
        "a. Use CountVectorizer.\r\n",
        "\r\n",
        "b. Use TfidfVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8HauGx7fbBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfdb3186-38aa-455e-9fb0-95a6b61d85dc"
      },
      "source": [
        "# Use Count Vectorizer\r\n",
        "count_vectorizer = CountVectorizer(max_features=1000)\r\n",
        "count_vect_data_features = count_vectorizer.fit_transform(mydata2['text'])\r\n",
        "count_vect_data_features = count_vect_data_features.toarray() \r\n",
        "count_vect_data_features.shape"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub-KukeNfcXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d00696fd-80d0-421e-d55f-b9880511870d"
      },
      "source": [
        "# Use Tfidf Vectorizer\r\n",
        "tfidf_vectoriser = TfidfVectorizer(max_features=2000)\r\n",
        "tfidf_data_features = tfidf_vectoriser.fit_transform(mydata2['text'])\r\n",
        "tfidf_data_features = tfidf_data_features.toarray()\r\n",
        "tfidf_data_features.shape"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGS4IwDVfdN1"
      },
      "source": [
        "**5. Fit and evaluate model using both type of vectorization. (6+6 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBnxnY1CftB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b819395-7b55-4b3c-a13c-7ed2586625c8"
      },
      "source": [
        "labels = mydata2['airline_sentiment'].apply(lambda x:0 if x=='negative' else 1 )\r\n",
        "labels.dtype"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dtype('int64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tAp4z8Zw4Cu"
      },
      "source": [
        " Classifiers = [\r\n",
        "    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\r\n",
        "    DecisionTreeClassifier(),\r\n",
        "    RandomForestClassifier(n_estimators=200),\r\n",
        "    AdaBoostClassifier()]\r\n",
        "   "
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39YKyl4Hw3cm"
      },
      "source": [
        "# Fit and Evaluate Model using Count Vectorizer\r\n",
        "Accuracy=[]\r\n",
        "Model=[]\r\n",
        "features=[]\r\n",
        "\r\n",
        "def model_fit(Xtrain,ytrain,xtest,ytest,feature_type):\r\n",
        " \r\n",
        "\r\n",
        "  #Accuracy=[]\r\n",
        "  #Model=[]\r\n",
        "  for clf in Classifiers:\r\n",
        "\r\n",
        "    try:\r\n",
        "      fit = clf.fit(Xtrain,ytrain)\r\n",
        "      pred = fit.predict(xtest)\r\n",
        "    except Exception:\r\n",
        "      fit = clf.fit(Xtrain,ytrain)\r\n",
        "      pred = fit.predict(xtest)\r\n",
        "    accuracy = accuracy_score(pred,ytest)\r\n",
        "    Accuracy.append(accuracy)\r\n",
        "    Model.append(clf.__class__.__name__)\r\n",
        "    features.append(feature_type)\r\n",
        "    print('Accuracy of '+clf.__class__.__name__+' is '+str(accuracy)+ ':' + feature_type) \r\n",
        "    print(classification_report(pred,ytest))\r\n",
        "    \r\n",
        "    #Confusion Matrix\r\n",
        "    #cm = confusion_matrix(pred,ytest)\r\n",
        "    #plt.figure()\r\n",
        "    #plot_confusion_matrix(cm,cmap=plt.cm.Blues)\r\n",
        "   \r\n",
        "    #plot_confusion_matrix((clf,pred, ytest)\r\n",
        "    #plt.xticks(range(2), ['Negative', 'Positive'],color='black')\r\n",
        "    #plt.yticks(range(2), ['Negative', 'Positive'])\r\n",
        "    #plt.xlabel('Predicted Label')\r\n",
        "    #plt.ylabel('True Label')\r\n",
        "    #plt.show()\r\n"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAK461RWw25N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d39d11d-824d-4cfc-c07f-e1c9c86c3877"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(count_vect_data_features,labels, test_size=0.3, random_state=42)\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)\r\n",
        "print(y_train.shape)\r\n",
        "print(y_test.shape)\r\n"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10248, 1000)\n",
            "(4392, 1000)\n",
            "(10248,)\n",
            "(4392,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vz8xy4AfuOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8f4caf-cd5b-4337-beaa-ce3415e9731a"
      },
      "source": [
        "model_fit(X_train,y_train,X_test,y_test,'CountVectorizer')\r\n"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of LogisticRegression is 0.6407103825136612:CountVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.64      0.78      4392\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.64      4392\n",
            "   macro avg       0.50      0.32      0.39      4392\n",
            "weighted avg       1.00      0.64      0.78      4392\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of DecisionTreeClassifier is 0.7408925318761385:CountVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.81      0.80      2740\n",
            "           1       0.66      0.63      0.65      1652\n",
            "\n",
            "    accuracy                           0.74      4392\n",
            "   macro avg       0.72      0.72      0.72      4392\n",
            "weighted avg       0.74      0.74      0.74      4392\n",
            "\n",
            "Accuracy of RandomForestClassifier is 0.7975865209471766:CountVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84      2855\n",
            "           1       0.71      0.72      0.71      1537\n",
            "\n",
            "    accuracy                           0.80      4392\n",
            "   macro avg       0.78      0.78      0.78      4392\n",
            "weighted avg       0.80      0.80      0.80      4392\n",
            "\n",
            "Accuracy of AdaBoostClassifier is 0.7786885245901639:CountVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.80      0.83      3028\n",
            "           1       0.62      0.72      0.67      1364\n",
            "\n",
            "    accuracy                           0.78      4392\n",
            "   macro avg       0.74      0.76      0.75      4392\n",
            "weighted avg       0.79      0.78      0.78      4392\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dF9LyJoyTaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4d4634-c3d7-49f5-9098-1d02db49b75d"
      },
      "source": [
        "# Fit and Evaluate Model using Tfidf Vectorizer\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_data_features,labels, test_size=0.3, random_state=42)\r\n",
        "\r\n",
        "print(X_train.shape)\r\n",
        "print(X_test.shape)\r\n",
        "print(y_train.shape)\r\n",
        "print(y_test.shape)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10248, 2000)\n",
            "(4392, 2000)\n",
            "(10248,)\n",
            "(4392,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZMzdc-PyUVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97918806-a0fb-48f0-cf48-b698943fad5c"
      },
      "source": [
        "model_fit(X_train,y_train,X_test,y_test,'TfidfVectorizer');"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of LogisticRegression is 0.6407103825136612:TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.64      0.78      4392\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.64      4392\n",
            "   macro avg       0.50      0.32      0.39      4392\n",
            "weighted avg       1.00      0.64      0.78      4392\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of DecisionTreeClassifier is 0.735655737704918:TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.80      0.79      2759\n",
            "           1       0.65      0.63      0.64      1633\n",
            "\n",
            "    accuracy                           0.74      4392\n",
            "   macro avg       0.72      0.71      0.72      4392\n",
            "weighted avg       0.73      0.74      0.73      4392\n",
            "\n",
            "Accuracy of RandomForestClassifier is 0.8089708561020036:TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.82      0.86      3093\n",
            "           1       0.65      0.78      0.71      1299\n",
            "\n",
            "    accuracy                           0.81      4392\n",
            "   macro avg       0.77      0.80      0.78      4392\n",
            "weighted avg       0.83      0.81      0.81      4392\n",
            "\n",
            "Accuracy of AdaBoostClassifier is 0.7773224043715847:TfidfVectorizer\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.79      0.84      3152\n",
            "           1       0.58      0.74      0.65      1240\n",
            "\n",
            "    accuracy                           0.78      4392\n",
            "   macro avg       0.73      0.77      0.74      4392\n",
            "weighted avg       0.80      0.78      0.78      4392\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysJfwiIxzLTF"
      },
      "source": [
        "myresult = pd.DataFrame({'Model': Model, 'Accuracy': Accuracy, 'Vectorizer': features})\r\n",
        "myresult = myresult[['Model', 'Accuracy', 'Vectorizer']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV3cBtOpzsJe"
      },
      "source": [
        "myresult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTgsl2QJ0PXe"
      },
      "source": [
        "The RandomForestClassifier has the highest accuracy and F1 scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNKDaBgm0OBG"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_data_features,labels, test_size=0.3, random_state=42)\r\n",
        "X_train.shape\r\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQVuO7-Zf3cO"
      },
      "source": [
        "forest = RandomForestClassifier(n_estimators=200, n_jobs=4)\r\n",
        "forest = forest.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5rFZh1D0js1"
      },
      "source": [
        "results = forest.predict(X_test)\r\n",
        "accuracy = accuracy_score(results,y_test)\r\n",
        "print(accuracy)\r\n",
        "#print(np.mean(cross_val_score(forest, tfidf_data_features, cv=10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_5KdHaU1L6W"
      },
      "source": [
        "**6. Summarize your understanding of the application of Various Pre-processing and Vectorization and\r\n",
        "performance of your model on this dataset. (8 Marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyTslbGW1MSd"
      },
      "source": [
        "The dataset reviews were converted to integers (0,1)\r\n",
        "\r\n",
        "I pre-processed the text column by doing the following:\r\n",
        "*   All HTML tags were removed using BeautifulSoup package because the data was acquired from the web\r\n",
        "*   The numbers and special characters were removed and replaced with spaces with the help of regular expression \r\n",
        "*   The contraction package converted words, reduced and/or combined them by dropping the apostrophe to the their full words\r\n",
        "*   **Single words were Tokenized after the removal of special characters and punctuations (instead of before) to avoid ending up with gibberish (combination of syllables with no meaning) after joining the words and converting back to text string in the dataframe**\r\n",
        "*   Stopwords from the tokenized text were removed and subsequently, converted to lowercase\r\n",
        "*   The tokenized words were Lemmatized\r\n",
        " \r\n",
        "Because models can only be run on integers, CountVectorizer and TfidfVectorizer were deployed to convert words to numbers.\r\n",
        "\r\n",
        "This model was built and evaluated using the following:  \r\n",
        "*   LogisticRegression\r\n",
        "*   DecisionTreeClassifier\r\n",
        "*   RandomForestClassifier\r\n",
        "*   AdaBoostClassifier\r\n",
        "\r\n",
        "The RadomForestClassifier produced the highest accuracy of approximately 81% using the TfidfVectorizer. \r\n",
        "\r\n",
        "To increase the accuracy of this model, i tuned the hyperparameters of the RandomForest Classifier model and increased the number of features from 1000 to 2000.\r\n"
      ]
    }
  ]
}